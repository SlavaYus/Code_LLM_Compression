{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d52d625b-eb3b-4f41-a443-8a3cabdd4fa8",
   "metadata": {},
   "source": [
    "# Обзор кодовых моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d01d2bd-3d42-4638-88b9-859055d48dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/trinity/home/team16/workspace/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/trinity/home/team16/workspace/.local/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8712cf19-73e2-4cb0-bc77-81c6e49657b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9048804-2ba2-40f7-b615-1b938a295716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/trinity/home/team16/workspace/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-10 16:51:08 arg_utils.py:766] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 08-10 16:51:08 config.py:820] Chunked prefill is enabled with max_num_batched_tokens=1024.\n",
      "INFO 08-10 16:51:08 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='deepseek-ai/deepseek-coder-7b-instruct', speculative_config=None, tokenizer='deepseek-ai/deepseek-coder-7b-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=65536, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=deepseek-ai/deepseek-coder-7b-instruct, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-10 16:51:10 model_runner.py:720] Starting to load model deepseek-ai/deepseek-coder-7b-instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W810 16:51:10.722357482 socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [gn35.zhores]:37043 (errno: 97 - Address family not supported by protocol).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-10 16:51:10 weight_utils.py:225] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f4a22e72034fd79c5190d188ec5d9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-10 16:51:13 model_runner.py:732] Loading model weights took 12.5708 GB\n",
      "INFO 08-10 16:51:13 gpu_executor.py:102] # GPU blocks: 7453, # CPU blocks: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.11s/it, est. speed input: 14.72 toks/s, output: 78.67 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\\n### Instruction:\\nСгенерируй код сортировки пузырьком на Python\\n### Response:\\n', Generated text: 'Вот пример кода сортировки пузырьком на Python:\\n\\n```python\\ndef bubbleSort(arr):\\n    n = len(arr)\\n\\n    for i in range(n-1):\\n        for j in range(0, n-i-1):\\n\\n            if arr[j] > arr[j+1]:\\n                arr[j], arr[j+1] = arr[j+1], arr[j]\\n\\narr = [64, 34, 25, 12, 22, 11, 90]\\n\\nbubbleSort(arr)\\n\\nprint(\"Sorted array is:\", arr)\\n```\\n\\nВ данном коде функция `bubbleSort` принимает список `arr`, который мы хотим отсортировать. Внутри этой функции мы выполняем основной алгоритм сортировки пузырьком: последовательно сравниваем пары элементов массива и меняем их местами, если они находятся в неправильном порядке. Это повторяется для каждой пары элементов до тех пор, пока мы не дойдем до конца массива и не выполним все сравнения.\\n\\nПосле того как функция `bubbleSort` отработает, на выходе мы получим отсортированный массив, который мы печатаем.\\n\\nЭтот сортировщик является простым, но он не очень эффективным для больших массивов, так как он имеет квадратичную сложность O(n^2).\\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Sample prompts.\n",
    "prompt = \"You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\\n### Instruction:\\nСгенерируй код сортировки пузырьком на Python. В ответ пиши код без дополнительных пояснений\\n### Response:\\n\"\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens = 512)\n",
    "\n",
    "# Create an LLM.\n",
    "llm = LLM(model=\"deepseek-ai/deepseek-coder-7b-instruct\", enforce_eager=True, max_num_batched_tokens = 1024)\n",
    "# Generate texts from the prompts. The output is a list of RequestOutput objects\n",
    "# that contain the prompt, generated text, and other information.\n",
    "outputs = llm.generate(prompt, sampling_params)\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa2d2701-80c7-42f4-8d47-daf8eb204338",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.82s/it, est. speed input: 60.97 toks/s, output: 80.20 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\\n### Instruction:\\nСгенерируй код сортировки пузырьком на Python. В ответ пиши код без дополнительных пояснений\\n### Response:\\n', Generated text: '```python\\ndef bubble_sort(nums):\\n    swapped = True\\n    while swapped:\\n        swapped = False\\n        for i in range(len(nums) - 1):\\n            if nums[i] > nums[i + 1]:\\n                nums[i], nums[i + 1] = nums[i + 1], nums[i]\\n                swapped = True\\n\\nnumbers = [64, 34, 25, 12, 22, 11, 90]\\nbubble_sort(numbers)\\nprint(numbers)\\n```\\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\\n### Instruction:\\nСгенерируй код сортировки пузырьком на Python. В ответ пиши код без дополнительных пояснений\\n### Response:\\n\"\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens = 512)\n",
    "outputs = llm.generate(prompt, sampling_params)\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87896328-383b-4168-b4d7-c1d0cba30197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.466666666666667"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "164 * 2 / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49113354-ab12-4784-ab5e-61192eb2fecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug 10 16:51:21 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:83:00.0 Off |                    0 |\n",
      "| N/A   46C    P0            257W /  400W |   73069MiB /  81920MiB |     84%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A    311101      C   /opt/conda/bin/python3                      73052MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d08dba-a6a0-4ad3-a9c0-9fdde70cf68e",
   "metadata": {},
   "source": [
    "## deepseek-coder-1.3b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75be6337-e476-43a5-974a-e0efd187560c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\\n### Instruction:\\nСгенерируй код сортировки пузырьком на Python\\n### Response:\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-instruct\", trust_remote_code=True)\n",
    "messages=[\n",
    "    { 'role': 'user', 'content': \"Сгенерируй код сортировки пузырьком на Python\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")#.to(model.device)\n",
    "tokenizer.decode(inputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01eeac7a-645b-428a-bc70-8731a2f1f9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/trinity/home/team16/workspace/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/trinity/home/team16/workspace/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def bubble_sort(arr):\n",
      "    n = len(arr)\n",
      "\n",
      "    for i in range(n):\n",
      "        for j in range(0, n-i-1):\n",
      "            if arr[j] > arr[j+1]:\n",
      "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
      "\n",
      "    return arr\n",
      "\n",
      "# Тест\n",
      "arr = [64, 34, 25, 12, 22, 11, 90]\n",
      "print(\"Исходный массив: \", arr)\n",
      "print(\"Сортированный массив: \", bubble_sort(arr))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-instruct\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-instruct\", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\n",
    "messages=[\n",
    "    { 'role': 'user', 'content': \"Сгенерируй код сортировки пузырьком на Python\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "# tokenizer.eos_token_id is the id of <|EOT|> token\n",
    "outputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4581d553-5bb1-4a8f-bb63-48b2eeadfaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, tokenizer, inputs, outputs\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8169e8-8a70-41a8-9869-dc443ac6a505",
   "metadata": {},
   "source": [
    "## deepseek-coder-7b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc989762-2589-4d98-a964-f3b88324e56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.20it/s]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def bubble_sort(numbers):\n",
      "    for i in range(len(numbers)):\n",
      "        for j in range(len(numbers) - 1):\n",
      "            if numbers[j] > numbers[j + 1]:\n",
      "                numbers[j], numbers[j + 1] = numbers[j + 1], numbers[j]\n",
      "    return numbers\n",
      "\n",
      "numbers = [64, 34, 25, 12, 22, 11, 90]\n",
      "print(bubble_sort(numbers))\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-7b-instruct\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-7b-instruct\", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\n",
    "messages=[\n",
    "    { 'role': 'user', 'content': \"Сгенерируй код сортировки пузырьком на Python\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "# tokenizer.eos_token_id is the id of <|EOT|> token\n",
    "outputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54bb8614-3d97-430d-ab14-4558349bac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, tokenizer, inputs, outputs\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ddf4c4-4efb-4db3-b601-ec9034722182",
   "metadata": {},
   "source": [
    "## CodeLlama-7b-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aa56a1a-cc9e-4fa4-a7a3-91a773567a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.19s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Напиши код, который сортирует список алгоритмом buble sort.\n",
      "\n",
      "```python\n",
      "def bubble_sort(arr):\n",
      "    n = len(arr)\n",
      "    for i in range(n-1):\n",
      "        for j in range(0, n-i-1):\n",
      "            if arr[j] > arr[j+1]:\n",
      "                arr[j], arr[j+1] = arr[j+1], arr[j]\n",
      "    return arr\n",
      "```\n",
      "\n",
      "### Задание 3\n",
      "\n",
      "Напиши функ\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('codellama/CodeLlama-7b-Instruct-hf')\n",
    "tokenizer = AutoTokenizer.from_pretrained('codellama/CodeLlama-7b-Instruct-hf')\n",
    "model.eval()\n",
    "model.to('cuda')\n",
    "\n",
    "code_generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Generate code for an input string\n",
    "input_string = \"Напиши код, который сортирует список алгоритмом buble sort\"\n",
    "with torch.no_grad():\n",
    "    generated_code = code_generator(input_string, max_length=128)[0]['generated_text']\n",
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b8d6321-ab56-4914-9023-6f9a56d55b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, tokenizer, generated_code\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b960bc-4874-4257-ad20-ee7e79fe391e",
   "metadata": {},
   "source": [
    "## codegeex4-all-9b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feba7225-1362-4759-a961-3a089770a521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/trinity/home/team16/workspace/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.66it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids device = cpu\n",
      "attention_mask device = cpu\n",
      "position_ids device = cpu\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('THUDM/codegeex4-all-9b',trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('THUDM/codegeex4-all-9b', trust_remote_code=True)\n",
    "model.eval()\n",
    "model.to('cuda')\n",
    "\n",
    "system = '<|system|>\\n Ты - гениальный программист, который помогает разработчикам писать код на языке Python. Ты всегда пишешь корректный и отформатированный код по нужному запросуn<|user|>\\n'\n",
    "prompts = [\n",
    "    'Сгенерируй код, который сортирует список алгоритмом buble sort. В ответ выдай только программу на языке Python  без пояснений'\n",
    "]\n",
    "inputs = tokenizer([system + prompt + '\\n<|assistant|>\\n' for prompt in prompts], return_tensors=\"pt\", padding=True)\n",
    "model.to('cuda')\n",
    "for k, v in inputs.items(): \n",
    "    inputs[k].to(model.device)\n",
    "for k, v in inputs.items(): \n",
    "    print(f'{k} device = {v.device}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    generation_output = model.generate(\\\n",
    "        input_ids = inputs['input_ids'].cuda(),\\\n",
    "        attention_mask = inputs['attention_mask'].cuda(),\\\n",
    "        position_ids = inputs['position_ids'].cuda(),\\\n",
    "        use_cache = False,\\\n",
    "        return_dict_in_generate=True,\\\n",
    "        output_scores=False,\\\n",
    "        max_new_tokens = 128,\\\n",
    "        do_sample = True,\\\n",
    "        top_p = 0.95,\\\n",
    "        repetition_penalty = 1.1,\\\n",
    "        temperature = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f86d6d5-c0fd-4228-bb63-85a33c5bb40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[gMASK] <sop> <|system|> \\n Ты - гениальный программист, который помогает разработчикам писать код на языке Python. Ты всегда пишешь корректный и отформатированный код по нужному запросуn <|user|> \\nСгенерируй код, который сортирует список алгоритмом buble sort. В ответ выдай только программу на языке Python  без пояснений\\n <|assistant|> \\n```python\\ndef bubble_sort(arr):\\n    n = len(arr)\\n    for i in range(n):\\n        for j in range(0, n-i-1):\\n            if arr[j] > arr[j+1]:\\n                arr[j], arr[j+1] = arr[j+1], arr[j]\\n    return arr\\n\\n# Пример использования:\\nunsorted_list = [64, 34, 25, 12, 22, 11, 90]\\nsorted_list = bubble_sort(unsorted_list)\\nprint(\"Отсортированный список:\", sorted_list)\\n```\\n <|endoftext|>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(generation_output.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d679f9d-647c-4fb2-ba88-0859344850c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, tokenizer, inputs, generation_output\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73202855-faca-4fdf-ae0a-65d05678c1b7",
   "metadata": {},
   "source": [
    "## Codegemma-2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df85b041-0ad2-4792-a8ce-1d952f7dc22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e403ea66b9400591a85591b4a1b61b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Напиши код, который сортирует список алгоритмом buble sort.py\n",
      "<|fim_prefix|><|fim_suffix|><|fim_middle|>def bubble_sort(arr):\n",
      "    for i in range(len(arr)):\n",
      "        for j in range(len(arr) - i - 1):\n",
      "            if arr[j] > arr[j + 1]:\n",
      "                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n",
      "    return arr\n",
      "\n",
      "arr = [5, 2, 8, 1, 3]\n",
      "print(bubble_sort(arr))\n",
      "<|file_separator|><eos>\n"
     ]
    }
   ],
   "source": [
    "from transformers import GemmaTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = GemmaTokenizer.from_pretrained(\"google/codegemma-2b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/codegemma-2b\")\n",
    "\n",
    "input_text = \"Напиши код, который сортирует список алгоритмом buble sort\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=400)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4319724-a89f-42eb-a4af-4a7bdf000e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, tokenizer, generated_code\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220342b2-5eeb-4267-9ace-bc518551484f",
   "metadata": {},
   "source": [
    "## Codegemma-7b instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3159690-07ca-49e7-9bd5-3a9442ebf4d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a4eac5b377468483689ce80c175e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Напиши код, который сортирует список алгоритмом buble sort.\n",
      "\n",
      "Напиши код, который сортирует список алгоритмом quick sort.\n",
      "\n",
      "Напиши код, который сортирует список алгоритмом merge sort.\n",
      "\n",
      "Напиши код, который сортирует список алгоритмом heap sort.\n",
      "\n",
      "Напиши код, который сортирует список алгоритмом insertion sort.\n",
      "\n",
      "Напиши код, который сортирует список алгоритмом selection sort.\n",
      "\n",
      "Напиши код, который сортирует список алгоритмом shell sort.\n",
      "\n",
      "Напиши код, который сортирует список алгоритмом counting sort.\n",
      "\n",
      "Напиши код, который сортирует список алгоритмом radix sort.\n",
      "\n",
      "Напиши код, который сортирует список алгоритмом bucket sort.\n",
      "\n",
      "Напиши код, который сортирует список алгоритмом gnome sort.\n",
      "\n",
      "Напиши код, который сортирует список алгоритмом comb sort.\n",
      "\n",
      "Напиши код, который сортирует список алгоритмом odd-even sort.\n",
      "\n",
      "Напиши код, который сортирует список алгоритмом pancake sort.\n",
      "\n",
      "Напиши код, который сортирует список алгоритмом shaker sort.\n",
      "\n",
      "Напиши код, который сортирует список алгоритмом brick sort.\n",
      "\n",
      "Напиши код, который сортирует список алгоритмом brick sort.\n",
      "\n",
      "Напиши код, который сортирует список алгоритмом brick sort.\n",
      "\n",
      "Напиши код, который сортирует список алгоритмом brick sort.\n",
      "\n",
      "Напиши код, который сортирует список алгоритмом brick sort.\n",
      "\n",
      "Напиши код, который сортирует список алгоритмом brick sort.\n",
      "\n",
      "Напиши код, который сортирует список алгоритмом brick sort.\n",
      "\n",
      "Напиши код, который сортирует список алгоритмом brick sort.\n",
      "\n",
      "Напиши код, который сортирует список алгоритмом brick sort.\n",
      "\n",
      "Напиши код, который сортирует список алгоритмом brick sort.\n",
      "\n",
      "Напиши код, который сортирует список алгоритмом\n"
     ]
    }
   ],
   "source": [
    "from transformers import GemmaTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = GemmaTokenizer.from_pretrained(\"google/codegemma-7b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/codegemma-7b\")\n",
    "\n",
    "input_text = \"Напиши код, который сортирует список алгоритмом buble sort\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=400)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9da6e018-f631-4fa9-be26-14bcddb2eec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.07it/s]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russian prompt. Generation = Напиши код, который сортирует список алгоритмом buble sort\n",
      "English prompt. Generation = Write a function to sort an array with buble sort algorithm\n",
      "\n",
      "`````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('google/codegemma-7b-it')\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/codegemma-7b-it')\n",
    "model.eval()\n",
    "model.to('cuda')\n",
    "\n",
    "code_generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Generate code for an input string\n",
    "input_string = \"Напиши код, который сортирует список алгоритмом buble sort\"\n",
    "with torch.no_grad():\n",
    "    generated_code = code_generator(input_string, max_length=128)[0]['generated_text']\n",
    "print(f'Russian prompt. Generation = {generated_code}')\n",
    "\n",
    "# Generate code for an input string\n",
    "input_string = \"Write a function to sort an array with buble sort algorithm\"\n",
    "with torch.no_grad():\n",
    "    generated_code = code_generator(input_string, max_length=128)[0]['generated_text']\n",
    "print(f'English prompt. Generation = {generated_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "597f1c3b-c475-47cd-bbab-daaf8c09387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, tokenizer, generated_code\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc72ee5b-3f46-43ec-81c6-90c2ce51cb84",
   "metadata": {},
   "source": [
    "## Gemma-2b instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19e9dd79-fbaa-449a-9e56-62d023e7f050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [00:45<00:00, 22.90s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.46s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russian prompt. Generation = Напиши код, который сортирует список алгоритмом buble sort\n",
      "English prompt. Generation = Write a function to sort an array with buble sort algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm algorithm\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('google/gemma-2b-it')\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/gemma-2b-it')\n",
    "model.eval()\n",
    "model.to('cuda')\n",
    "\n",
    "code_generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Generate code for an input string\n",
    "input_string = \"Напиши код, который сортирует список алгоритмом buble sort\"\n",
    "with torch.no_grad():\n",
    "    generated_code = code_generator(input_string, max_length=128)[0]['generated_text']\n",
    "print(f'Russian prompt. Generation = {generated_code}')\n",
    "\n",
    "# Generate code for an input string\n",
    "input_string = \"Write a function to sort an array with buble sort algorithm\"\n",
    "with torch.no_grad():\n",
    "    generated_code = code_generator(input_string, max_length=128)[0]['generated_text']\n",
    "print(f'English prompt. Generation = {generated_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2f1be54-9fe7-4402-9239-35da083db6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, tokenizer, generated_code\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29a20fe-f0f6-40bf-8c83-873f6a96ea20",
   "metadata": {},
   "source": [
    "## Gemma-7b instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0932a275-4cb4-4c39-8a5b-f0f70fb5d0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/trinity/home/team16/workspace/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.94s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russian prompt. Generation = Напиши код, который сортирует список алгоритмом buble sort\n",
      "English prompt. Generation = Write a function to sort an array with buble sort algorithm\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('google/gemma-7b-it')\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/gemma-7b-it')\n",
    "model.eval()\n",
    "model.to('cuda')\n",
    "\n",
    "code_generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Generate code for an input string\n",
    "input_string = \"Напиши код, который сортирует список алгоритмом buble sort\"\n",
    "with torch.no_grad():\n",
    "    generated_code = code_generator(input_string, max_length=128)[0]['generated_text']\n",
    "print(f'Russian prompt. Generation = {generated_code}')\n",
    "\n",
    "# Generate code for an input string\n",
    "input_string = \"Write a function to sort an array with buble sort algorithm\"\n",
    "with torch.no_grad():\n",
    "    generated_code = code_generator(input_string, max_length=128)[0]['generated_text']\n",
    "print(f'English prompt. Generation = {generated_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d10ae00-1eec-4d8f-afde-176b24f45c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, tokenizer, generated_code\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
