{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "DATASET = \"iamtarun/python_code_instructions_18k_alpaca\"\n",
    "\n",
    "####### SHORTEND_LLM #######\n",
    "MODEL_NAME=\"deepseek-ai/LASER_PRUNED_LORA_deepseek-coder-7b-instruct-v1.5\" # path to model\n",
    "NUM_CALIB_DATA=10 # Что-то с датасетом\n",
    "NUM_PRUNED_BLOCKS=6 # Количество блоков для прунинга с минимальной метрикой\n",
    "OUTPUT_SENSITIVITY=f\"cache/shortend_llm/output_block_sensitivity/{MODEL_NAME}/taylor_n{NUM_CALIB_DATA}\"\n",
    "OUTPUT_PRUNE=f\"cache/shortend_llm/output_prune/{MODEL_NAME}/taylor_n{NUM_CALIB_DATA}/rm_{NUM_PRUNED_BLOCKS}_blocks\"\n",
    "OUTPUT_TUNE=f\"cache/shortend_llm/output_tune/{MODEL_NAME}/taylor_n{NUM_CALIB_DATA}/rm_{NUM_PRUNED_BLOCKS}_blocks\"\n",
    "CHECKPOINT_PATH = f\"cache/shortend_llm/output_tune/{MODEL_NAME}/checkpoints\"\n",
    "NORM_POWER = 1\n",
    "WEIGHT_REDUCTION = \"sum\"\n",
    "BLOCK_REDUCTION = \"sum\"\n",
    "MODEL_TYPE = \"pretrain\"\n",
    "NUM_CALIB_DATA = 10\n",
    "MAX_SEQ_LEN = 256\n",
    "BATCH_SIZE_SHORTEND = 10\n",
    "# DATASET_PATH = 'python_code_instructions_18k_alpaca_ru_prompt'\n",
    "UPDATE_FIRST_STEP_SHORTEND = True\n",
    "\n",
    "######## SVD LLM #########\n",
    "PROFILING_MAT_PATH = None # Путь с предрасчетом значений для SVD\n",
    "WHITENING_NSAMPLES = 256\n",
    "RATIO = 0.6\n",
    "MAX_LEN_WHITENING = 256\n",
    "OUTPUT_SVD_SAVE = f\"cache/svd_llm/output_whitening/{MODEL_NAME.split('/')[0]}\"\n",
    "MODEL_ADD_PATH  = f\"/{MODEL_NAME.split('/')[1]}_{RATIO}_{MAX_LEN_WHITENING}.pt\"\n",
    "\n",
    "####### LORA #######\n",
    "LORA_R = 8 # Rank of lora \n",
    "LORA_ALPHA = 32\n",
    "NUM_EPOCHS = 2 # Number of fine tune epochs\n",
    "LEARNING_RATE = 2e-4\n",
    "BATCH_SIZE = 84\n",
    "MICRO_BATCH_SIZE = 4 # number of different forwards before accumulation grads in each batch \n",
    "CUTOFF_LENGTH = 500\n",
    "TRAIN_ON_INPUTS = False # Should model learn on input text also or not\n",
    "\n",
    "LORA_TARGET_MODULES = \"q_v_proj,q_u_proj,k_v_proj,k_u_proj,v_u_proj,\\\n",
    "v_v_proj,o_u_proj,o_v_proj,gate_u_proj,gate_v_proj,down_u_proj,down_v_proj,up_u_proj,up_v_proj,\\\n",
    "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\"\n",
    "\n",
    "LORA_DROPOUT = 0.05\n",
    "VAL_SIZE = 1000\n",
    "EVAL_STEPS = 50\n",
    "DATA_PATH = 'LORA/python_code_instructions_18k_alpaca_ru'\n",
    "OUTPUT_DIR = f\"../cache/lora/lora_finetuned_{MODEL_NAME.split('/')[1]}/\"\n",
    "\n",
    "####### HUMAN_EVAL ########\n",
    "MAX_NEW_TOKENS = 512\n",
    "NUM_SAMPLES_PER_TASK = 1\n",
    "SAVE_METRIC_PATH = f\"{MODEL_NAME.split('/')[1]}_test_{MAX_NEW_TOKENS}_{NUM_SAMPLES_PER_TASK}_samples.jsonl\"\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/trinity/home/team16/workspace/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/trinity/home/team16/workspace/.local/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/trinity/home/team16/workspace/.local/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import transformers\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from datasets import load_dataset, load_from_disk, DatasetDict\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from final_combination.utils_shortend import *\n",
    "from final_combination.utils_svd_llm import *\n",
    "from typing import Iterable, Dict\n",
    "from final_combination.LLMPruner.peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import AutoConfig\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from LORA.Prompter import Prompter, ZeroPrompter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_name_or_path\": \"deepseek-ai/deepseek-coder-7b-instruct-v1.5\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"bos_token_id\": 100000,\n",
       "  \"eos_token_id\": 100015,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 11008,\n",
       "  \"max_position_embeddings\": 4096,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 30,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.35.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 102400\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHORTEND LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(\n",
    "    base_model=None,\n",
    "    ckpt=None,\n",
    "    lora_ckpt=None,\n",
    "    tokenizer=None,\n",
    "    model_type=\"pretrain\",\n",
    "    device=\"cuda\",\n",
    "    fix_decapoda_config=False,\n",
    "    use_bfloat=False,\n",
    "):\n",
    "    tokenizer = base_model if tokenizer is None else tokenizer\n",
    "    if model_type == \"pretrain\":\n",
    "        config = AutoConfig.from_pretrained(base_model)\n",
    "        if \"gptq\" in base_model.lower():\n",
    "            from auto_gptq import AutoGPTQForCausalLM\n",
    "\n",
    "            model = AutoGPTQForCausalLM.from_quantized(\n",
    "                base_model,\n",
    "                use_safetensors=True,\n",
    "                trust_remote_code=True,\n",
    "                use_triton=False,\n",
    "                quantize_config=None,\n",
    "            )\n",
    "            tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
    "        elif (\n",
    "            \"LlamaForCausalLM\" in config.__getattribute__(\"architectures\")\n",
    "            and \"llama-3\" not in base_model.lower()\n",
    "        ):\n",
    "            model = LlamaForCausalLM.from_pretrained(base_model, low_cpu_mem_usage=True)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(tokenizer) # ЗАМЕНИЛ НА ЭТО! ПРОВЕРИТЬ\n",
    "            # tokenizer = LlamaTokenizer.from_pretrained(tokenizer)\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                base_model, low_cpu_mem_usage=True\n",
    "            )\n",
    "            tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
    "    elif model_type in [\"pruneLLM\", \"tune_pruneLLM\"]:\n",
    "        pruned_dict = torch.load(ckpt, map_location=\"cpu\")\n",
    "        model = pruned_dict[\"model\"]\n",
    "        tokenizer = pruned_dict[\"tokenizer\"]\n",
    "        if model_type == \"tune_pruneLLM\":\n",
    "            model = PeftModel.from_pretrained(\n",
    "                model, lora_ckpt, torch_dtype=torch.float16, low_cpu_mem_usage=True\n",
    "            )\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    description = \"Model Type: {}\\n Base: {} \\n Pruned: {}\\n LORA: {}\".format(\n",
    "        model_type, base_model, ckpt, lora_ckpt\n",
    "    )\n",
    "\n",
    "    if fix_decapoda_config:\n",
    "        # unwind broken decapoda-research config\n",
    "        tokenizer.pad_token_id = 0\n",
    "    model = set_model_device_evalmode(model, device, fix_decapoda_config, use_bfloat)\n",
    "\n",
    "    return model, tokenizer, description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Анализируем блоки LLM на основе метрики Taylor. Мериется метрика по датасету, выбираются не нужные слои и все сохраняется. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "os.makedirs(OUTPUT_SENSITIVITY, exist_ok=True)\n",
    "\n",
    "norm_power = NORM_POWER\n",
    "weight_reduction = WEIGHT_REDUCTION\n",
    "block_reduction = BLOCK_REDUCTION\n",
    "result_csv_weight = os.path.join(OUTPUT_SENSITIVITY, \"weight_score.csv\")\n",
    "result_csv_block = os.path.join(OUTPUT_SENSITIVITY, \"block_score_all.csv\")\n",
    "result_csv_block_detail = os.path.join(OUTPUT_SENSITIVITY, \"block_score_detail.csv\")\n",
    "result_csv_block_sort = os.path.join(OUTPUT_SENSITIVITY, \"block_score_sorted.csv\")\n",
    "block_order_path = os.path.join(OUTPUT_SENSITIVITY, \"block_order.csv\")\n",
    "\n",
    "if not os.path.exists(block_order_path) or UPDATE_FIRST_STEP_SHORTEND:\n",
    "    model, tokenizer, description = get_model(\n",
    "        base_model=MODEL_NAME,\n",
    "        ckpt=CHECKPOINT_PATH,\n",
    "        lora_ckpt=CHECKPOINT_PATH,\n",
    "        tokenizer=None,\n",
    "        model_type=MODEL_TYPE,\n",
    "        device=DEVICE,\n",
    "        fix_decapoda_config=False,\n",
    "        use_bfloat=False,\n",
    "    )\n",
    "    \n",
    "    ### ЗАМЕНИТЬ!\n",
    "    example_prompts = get_examples(\n",
    "        dataset=DATASET,\n",
    "        tokenizer=tokenizer,\n",
    "        n_samples=NUM_CALIB_DATA ,\n",
    "        seq_len=MAX_SEQ_LEN,\n",
    "        field_name=\"prompt\",\n",
    "        add_bos_to_every=False,\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    print(\"Do forward to collect gradient information\")\n",
    "    salience_dict = {}\n",
    "    for i in range(0, example_prompts.size(0), BATCH_SIZE_SHORTEND):\n",
    "        example_prompts_tmp = example_prompts[i : i + BATCH_SIZE_SHORTEND]\n",
    "        loss = model(example_prompts_tmp, labels=example_prompts_tmp).loss\n",
    "        loss.backward()\n",
    "        for k, param in model.named_parameters():\n",
    "            if param.requires_grad and \"weight\" in k and \"embed_tokens\" not in k:\n",
    "                salience = param * param.grad\n",
    "                salience = salience.data.clone().float()\n",
    "    \n",
    "                if k not in salience_dict.keys():\n",
    "                    salience_dict[k] = salience\n",
    "                else:\n",
    "                    salience_dict[k] += salience\n",
    "        model.zero_grad()\n",
    "    \n",
    "    # Compute scores of weight matrices -> Collec them\n",
    "    block_info = {}\n",
    "    with open(result_csv_weight, \"w\") as logfile:\n",
    "        logwriter = csv.writer(logfile, delimiter=\",\")\n",
    "        logwriter.writerow([\"weight_name\", \"weight_score\"])\n",
    "        for k, param in model.named_parameters():\n",
    "            if param.requires_grad and \"weight\" in k and \"embed_tokens\" not in k:\n",
    "                block_idx = \".\".join(k.split(\".\")[:3])  # 'model.layers.i'\n",
    "                if \"proj\" in k or \"lm_head\" in k:  # output_dim x input_dim\n",
    "                    weight_imp = (\n",
    "                        salience_dict[k].abs().pow(norm_power).sum(1)\n",
    "                    )  # [output_dim]\n",
    "                elif \"norm\" in k:  # [output_dim]\n",
    "                    weight_imp = salience_dict[k].abs().pow(norm_power)\n",
    "    \n",
    "                if weight_reduction == \"sum\":\n",
    "                    weight_imp = weight_imp.sum(dim=0)\n",
    "                elif weight_reduction == \"mean\":\n",
    "                    weight_imp = weight_imp.mean(dim=0)\n",
    "                elif weight_reduction == \"max\":\n",
    "                    weight_imp = weight_imp.max(dim=0)[0]\n",
    "                elif weight_reduction == \"prod\":\n",
    "                    weight_imp = torch.prod(weight_imp, dim=0)\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "    \n",
    "                weight_imp = weight_imp.item()\n",
    "                logwriter.writerow([k, weight_imp])\n",
    "                # print([k, weight_imp])\n",
    "                if block_idx not in block_info.keys():\n",
    "                    block_info[block_idx] = [weight_imp]\n",
    "                else:\n",
    "                    block_info[block_idx].append(weight_imp)\n",
    "    \n",
    "    # Compute block-level importance\n",
    "    block_info_summary = {}\n",
    "    with open(result_csv_block, \"w\") as logfile, open(\n",
    "        result_csv_block_detail, \"w\"\n",
    "    ) as logfile_detail:\n",
    "        logwriter = csv.writer(logfile, delimiter=\",\")\n",
    "        logwriter.writerow([\"block_name\", \"block_score\"])\n",
    "        logwriter_detail = csv.writer(logfile_detail, delimiter=\",\")\n",
    "        logwriter_detail.writerow([\"block_name\", \"all_weight_scores\"])\n",
    "        for k, v in block_info.items():\n",
    "            # print(k, v)\n",
    "            logwriter_detail.writerow([k] + v)\n",
    "    \n",
    "            block_imp = torch.tensor(v)\n",
    "            if block_reduction == \"sum\":\n",
    "                block_imp = block_imp.sum(dim=0)\n",
    "            elif block_reduction == \"mean\":\n",
    "                block_imp = block_imp.mean(dim=0)\n",
    "            elif block_reduction == \"max\":\n",
    "                block_imp = block_imp.max(dim=0)[0]\n",
    "            elif block_reduction == \"prod\":\n",
    "                block_imp = torch.prod(block_imp, dim=0)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "    \n",
    "            block_imp = block_imp.item()\n",
    "            logwriter.writerow([k, block_imp])\n",
    "            block_info_summary[k] = block_imp\n",
    "    \n",
    "    for k in [\"model.norm.weight\", \"lm_head.weight\"]:\n",
    "        if k in block_info_summary:\n",
    "            del block_info_summary[k]\n",
    "    sorted_items = sorted(block_info_summary.items(), key=lambda x: x[1])\n",
    "    block_order = []\n",
    "    with open(result_csv_block_sort, \"w\") as logfile:\n",
    "        logwriter = csv.writer(logfile, delimiter=\",\")\n",
    "        logwriter.writerow([\"rank\", \"block_name\", \"block_score\", \"block_index\"])\n",
    "        for rank, (key, value) in enumerate(sorted_items, start=1):\n",
    "            logwriter.writerow([rank, key, value, key.split(\".\")[-1]])\n",
    "            # print([rank, key, value, key.split(\".\")[-1]])\n",
    "            block_order.append(int(key.split(\".\")[-1]))\n",
    "    \n",
    "    with open(block_order_path, \"w\") as logfile_order:\n",
    "        logwriter_order = csv.writer(logfile_order, delimiter=\",\")\n",
    "        logwriter_order.writerow(block_order)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(f\"use the precomputed results at {block_order_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model, tokenizer, description\n",
    "except:\n",
    "    pass\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убираем сохраненные слои с наименьшой метрикой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "\n",
    "model_orig, tokenizer, description = get_model(\n",
    "    base_model=MODEL_NAME,\n",
    "    ckpt=CHECKPOINT_PATH,\n",
    "    lora_ckpt=CHECKPOINT_PATH,\n",
    "    tokenizer=None,\n",
    "    model_type=MODEL_TYPE,\n",
    "    device=DEVICE,\n",
    "    fix_decapoda_config=False,\n",
    "    use_bfloat=False,\n",
    ")\n",
    "\n",
    "os.makedirs(OUTPUT_PRUNE, exist_ok=True)\n",
    "\n",
    "# Load the precomputed block unimportance order\n",
    "unimportance_order = []\n",
    "with open(block_order_path, \"r\") as file:\n",
    "    unimportance_order = [int(i) for i in str(next(file).strip()).split(\",\")]\n",
    "\n",
    "if not False:\n",
    "    last_block_index = model_orig.config.num_hidden_layers - 1\n",
    "    keep_block_info = [\n",
    "        0,\n",
    "        1,\n",
    "        2,\n",
    "        3,\n",
    "        last_block_index - 1,\n",
    "        last_block_index,\n",
    "    ]  # to keep first and last few blocks unpruned\n",
    "    unimportance_order = [\n",
    "        idx for idx in unimportance_order if idx not in keep_block_info\n",
    "    ]\n",
    "\n",
    "# Block-level pruning\n",
    "model = get_block_pruned_network(\n",
    "    model_orig,\n",
    "    unimportance_order=unimportance_order,\n",
    "    num_pruned_blocks=NUM_PRUNED_BLOCKS,\n",
    "    device=DEVICE,\n",
    "    fix_decapoda_config=False,\n",
    "    use_bfloat=False,\n",
    ")\n",
    "\n",
    "# Save\n",
    "torch.save({'model': model, 'tokenizer': tokenizer}, OUTPUT_PRUNE + \"/model_shortend_llm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del example_prompts\n",
    "del description\n",
    "del model\n",
    "del tokenizer\n",
    "del salience_dict\n",
    "del sorted_items\n",
    "del unimportance_order\n",
    "del loss\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD-LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/trinity/home/team16/workspace/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0dc4fa0bf844fbe824ec2571f598ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pruned_dict = torch.load(OUTPUT_PRUNE + \"/model_shortend_llm.pt\", map_location=DEVICE)\n",
    "tokenizer, model = pruned_dict['tokenizer'], pruned_dict['model']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собираем примеры для прогона их через через нейросеть и измеряем output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [06:01<00:00,  1.41s/it]\n",
      "100%|██████████| 30/30 [00:03<00:00,  8.02it/s]\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        module.raw_scaling_diag_matrix = 0\n",
    "        module.register_forward_hook(hook)\n",
    "\n",
    "data = load_dataset(DATASET)\n",
    "train_val = data[\"train\"]\n",
    "\n",
    "\n",
    "messages=[\n",
    "    {'role': 'user', 'content': train_val[i]['prompt']} for i in range(len(train_val))\n",
    "]\n",
    "\n",
    "inputs = []\n",
    "\n",
    "for i in range(WHITENING_NSAMPLES): \n",
    "    messages=[{'role': 'user', 'content': train_val[i]['prompt']}]\n",
    "    curr_inp = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", max_length = MAX_LEN_WHITENING, padding = True, truncation = True).to(model.device)\n",
    "    inputs.append(curr_inp)\n",
    "\n",
    "set_seed(SEED)\n",
    "with torch.inference_mode(): \n",
    "    for item in tqdm(inputs):\n",
    "        model(item)\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        module._forward_hooks.clear()\n",
    "\n",
    "for i in trange(len(model.model.layers)):\n",
    "    subset = find_layers(model.model.layers[i])\n",
    "    for name in subset:\n",
    "        subset[name].raw_scaling_diag_matrix = subset[name].raw_scaling_diag_matrix.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Cholesky Decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: eigen scaling_diag_matrix is not positive!\n",
      "Warning: eigen scaling_diag_matrix is not positive!\n",
      "Warning: eigen scaling_diag_matrix is not positive!\n",
      "Warning: eigen scaling_diag_matrix is not positive!\n",
      "Warning: eigen scaling_diag_matrix is not positive!\n",
      "Warning: eigen scaling_diag_matrix is not positive!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:02<01:12,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: eigen scaling_diag_matrix is not positive!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [00:22<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: eigen scaling_diag_matrix is not positive!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:23<00:00,  1.28it/s]\n"
     ]
    }
   ],
   "source": [
    "profiling_mat = {}\n",
    "print(\"Start Cholesky Decomposition...\")\n",
    "for i in tqdm(range(len(model.model.layers))):\n",
    "    layer_profile = {}\n",
    "    subset = find_layers(model.model.layers[i])\n",
    "    for name in subset:\n",
    "        raw_scaling_diag_matrix = subset[name].raw_scaling_diag_matrix.double().to(DEVICE)\n",
    "        \n",
    "        try:\n",
    "            scaling_diag_matrix = torch.linalg.cholesky(raw_scaling_diag_matrix)\n",
    "        except Exception as e:\n",
    "            print(\"Warning: eigen scaling_diag_matrix is not positive!\")\n",
    "            eigenvalues = torch.linalg.eigvalsh(raw_scaling_diag_matrix)\n",
    "            raw_scaling_diag_matrix += (- eigenvalues[0] + 1e-6) * torch.eye(raw_scaling_diag_matrix.shape[0]).to(DEVICE)\n",
    "            scaling_diag_matrix = torch.linalg.cholesky(raw_scaling_diag_matrix)\n",
    "            eigenvalues = None\n",
    "            del eigenvalues\n",
    "            \n",
    "        layer_profile[name] = scaling_diag_matrix.cpu()\n",
    "        scaling_diag_matrix = raw_scaling_diag_matrix = subset[name].raw_scaling_diag_matrix = None\n",
    "        del scaling_diag_matrix, raw_scaling_diag_matrix, subset[name].raw_scaling_diag_matrix\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    profiling_mat[i] = layer_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применяем SVD разложение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [07:42<00:00, 15.41s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "for i in trange(len(model.model.layers)):\n",
    "    layer = model.model.layers[i]\n",
    "    subset = find_layers(layer)\n",
    "\n",
    "    svd_attn = SVD_LlamaAttention(config=model.config, ratio=RATIO)\n",
    "    svd_mlp = SVD_LlamaMLP(hidden_size=layer.hidden_size, intermediate_size=model.config.intermediate_size, hidden_act=model.config.hidden_act, ratio=RATIO)\n",
    "\n",
    "    for name in subset:\n",
    "        skip_this_layer = False\n",
    "        W = subset[name].weight.data.float()\n",
    "        dtype = W.dtype\n",
    "        scaling_diag_matrix = profiling_mat[i][name].cuda()\n",
    "        try:\n",
    "            scaling_matrix_inv = torch.linalg.inv(scaling_diag_matrix)\n",
    "        except Exception as e:\n",
    "            print(\"Warning: scaling_diag_matrix is not full rank!\")\n",
    "            scaling_diag_matrix += 1e-6 * torch.eye(scaling_diag_matrix.shape[0]).to(DEVICE)\n",
    "            try:\n",
    "                scaling_matrix_inv = torch.linalg.inv(scaling_diag_matrix)\n",
    "            except:\n",
    "                skip_this_layer = True\n",
    "        if not(skip_this_layer):        \n",
    "            scaling_diag_matrix = scaling_diag_matrix.float()\n",
    "            scaling_matrix_inv = scaling_matrix_inv.float()\n",
    "            W_scale = torch.matmul(W, scaling_diag_matrix)\n",
    "            \n",
    "            U, S, VT = torch.linalg.svd(W_scale, full_matrices=False)\n",
    "            \n",
    "            num_s_after_trunc = int(W.shape[0] * W.shape[1] * RATIO / (W.shape[0] + W.shape[1]))\n",
    "            \n",
    "            truc_s = S[:num_s_after_trunc]\n",
    "            truc_u = U[:, :num_s_after_trunc]\n",
    "            truc_v = torch.matmul(VT[:num_s_after_trunc, :], scaling_matrix_inv)\n",
    "            truc_sigma = torch.diag(truc_s)\n",
    "            \n",
    "            #### Replace Attn, MLP ####\n",
    "                \n",
    "            sqrtSigma = torch.sqrt(truc_sigma)\n",
    "            svd_u = torch.matmul(truc_u, sqrtSigma).cpu().to(dtype)\n",
    "            svd_v = torch.matmul(sqrtSigma, truc_v).cpu().to(dtype)\n",
    "            \n",
    "            if \"q_proj\" in name:\n",
    "                svd_attn.q_u_proj.weight.data = svd_u\n",
    "                svd_attn.q_v_proj.weight.data = svd_v\n",
    "            elif \"k_proj\" in name:\n",
    "                svd_attn.k_u_proj.weight.data = svd_u\n",
    "                svd_attn.k_v_proj.weight.data = svd_v\n",
    "            elif \"v_proj\" in name:\n",
    "                svd_attn.v_u_proj.weight.data = svd_u\n",
    "                svd_attn.v_v_proj.weight.data = svd_v\n",
    "            elif \"o_proj\" in name:\n",
    "                svd_attn.o_u_proj.weight.data = svd_u\n",
    "                svd_attn.o_v_proj.weight.data = svd_v\n",
    "                layer.self_attn =  svd_attn\n",
    "            elif \"gate_proj\" in name:\n",
    "                svd_mlp.gate_u_proj.weight.data = svd_u\n",
    "                svd_mlp.gate_v_proj.weight.data = svd_v\n",
    "            elif \"down_proj\" in name:\n",
    "                svd_mlp.down_u_proj.weight.data = svd_u\n",
    "                svd_mlp.down_v_proj.weight.data = svd_v\n",
    "            elif \"up_proj\" in name:\n",
    "                svd_mlp.up_u_proj.weight.data = svd_u\n",
    "                svd_mlp.up_v_proj.weight.data = svd_v\n",
    "                layer.mlp = svd_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num params after SVD compression = 4480897536\n"
     ]
    }
   ],
   "source": [
    "print(f'num params after SVD compression = {sum( [ np.prod(item.size())  for item in model.parameters() ])}')\n",
    "os.makedirs(OUTPUT_SVD_SAVE, exist_ok=True)\n",
    "torch.save({'model': model, 'tokenizer': tokenizer}, OUTPUT_SVD_SAVE+MODEL_ADD_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to excelude LASER or other special layers from fine tune\n",
    "# decoder_id_ls = [i for i in range(0,15)] + [i for i in range(18,30)]\n",
    "decoder_id_ls = [i for i in range(0,30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/trinity/home/team16/workspace/.local/lib/python3.10/site-packages/peft/utils/other.py:135: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 13,117,440 || all params: 5,197,870,221 || trainable%: 0.252361822097905\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aec605107ec469596b068267566d1c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17612 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max prompt length: 472\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f6fb94a6ce415ab4f990805afbb512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n",
      "/trinity/home/team16/workspace/.local/lib/python3.10/site-packages/accelerate/accelerator.py:451: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "/trinity/home/team16/workspace/.local/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "Detected kernel version 5.4.270, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/trinity/home/team16/workspace/.local/lib/python3.10/site-packages/transformers/trainer.py:2545: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/trinity/home/team16/workspace/.local/lib/python3.10/site-packages/transformers/trainer.py:2308: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='418' max='418' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [418/418 27:58, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Lora/python Code Instructions 18k Alpaca Ru Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.446500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.470408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.444700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.465488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.451600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.462208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.448000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.460686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# Load Pruned Model\n",
    "\n",
    "pruned_dict = torch.load(OUTPUT_SVD_SAVE+MODEL_ADD_PATH, map_location='cpu')\n",
    "tokenizer, model = pruned_dict['tokenizer'], pruned_dict['model']\n",
    "gradient_accumulation_steps = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "\n",
    "prompter = Prompter('alpaca')\n",
    "\n",
    "if DEVICE == 'cuda':\n",
    "    model.half()\n",
    "\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LENGTH,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < CUTOFF_LENGTH\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "    \n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    old = False\n",
    "    if old:\n",
    "        full_prompt = prompter.generate_prompt(\n",
    "            data_point[\"instruction\"],\n",
    "            data_point[\"input\"],\n",
    "            data_point[\"output\"],\n",
    "        )\n",
    "        \n",
    "        tokenized_full_prompt = tokenize(full_prompt)\n",
    "        if TRAIN_ON_INPUTS:\n",
    "            user_prompt = prompter.generate_prompt(\n",
    "                data_point[\"instruction\"], data_point[\"input\"]\n",
    "            )\n",
    "            tokenized_user_prompt = tokenize(\n",
    "                user_prompt, add_eos_token=False\n",
    "            )\n",
    "            user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "    \n",
    "            if False:\n",
    "                user_prompt_len -= 1\n",
    "    \n",
    "            tokenized_full_prompt[\"labels\"] = [\n",
    "                -100\n",
    "            ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
    "                user_prompt_len:\n",
    "            ]  # could be sped up, probably\n",
    "    else:\n",
    "        messages=[\n",
    "            { 'role': 'user', 'content': data_point['prompt']}\n",
    "        ]\n",
    "        #return_tensors=\"pt\"\n",
    "        tokenized_full_prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "        tokenized_full_prompt = re.sub(r'### Response:', '', tokenized_full_prompt)\n",
    "        tokenized_full_prompt = re.sub(r\"### Instruction:\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n",
    ", '', tokenized_full_prompt)\n",
    "        tokenized_full_prompt = re.sub(r'### Output:', '### Response:', tokenized_full_prompt)\n",
    "    return tokenize(tokenized_full_prompt)\n",
    "\n",
    "def split_and_tokenizer(test_data, tokenizer, seq_len, field_name):\n",
    "    test_ids = tokenizer(\"\\n\\n\".join(test_data[field_name]), return_tensors='pt').input_ids[0]\n",
    "    test_ids_batch = []\n",
    "    nsamples = test_ids.numel() // seq_len\n",
    "\n",
    "    test_set = []\n",
    "    for i in range(nsamples):\n",
    "        batch = test_ids[(i * seq_len):((i + 1) * seq_len)]\n",
    "        test_set.append({\n",
    "            'input_ids': batch,\n",
    "            'labels': batch\n",
    "        })\n",
    "    return test_set\n",
    "    \n",
    "# Prepare For LoRA\n",
    "model = prepare_model_for_int8_training(model)\n",
    "config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=LORA_TARGET_MODULES.split(\",\"),\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    layers_to_transform = decoder_id_ls,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()  \n",
    "\n",
    "# Load Train Dataset\n",
    "try:\n",
    "    data = load_dataset(DATASET)\n",
    "except:\n",
    "    data = DatasetDict({'train':load_from_disk(DATA_PATH)})\n",
    "    \n",
    "train_val = data[\"train\"].train_test_split(\n",
    "    test_size=VAL_SIZE, shuffle=True, seed=42\n",
    ")\n",
    "train_data = (\n",
    "    train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    ")\n",
    "\n",
    "input_lenghts = [len(x) for x in train_data[\"input_ids\"]]\n",
    "max_source_length = int(np.percentile(input_lenghts, 95))\n",
    "print(f\"Max prompt length: {max_source_length}\")\n",
    "\n",
    "val_data = {\n",
    "    DATA_PATH: train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt),\n",
    "}\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        warmup_steps=100,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        fp16=True,\n",
    "        logging_steps=10,\n",
    "        logging_first_step=True,\n",
    "        optim=\"adamw_torch\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=EVAL_STEPS,\n",
    "        save_steps=100,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        save_total_limit=20,\n",
    "        load_best_model_at_end=True,\n",
    "        ddp_find_unused_parameters=None,\n",
    "        group_by_length=False,\n",
    "        report_to=\"none\",\n",
    "        run_name=\"none\",\n",
    "        metric_for_best_model=\"{}_loss\".format(DATA_PATH),\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "    ),\n",
    ")\n",
    "model.config.use_cache = False\n",
    "old_state_dict = model.state_dict\n",
    "\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "\n",
    "model.state_dict = old_state_dict\n",
    "model.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model': model, 'tokenizer': tokenizer}, OUTPUT_DIR+MODEL_ADD_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HUMAN_EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2223184/2026392382.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pruned_dict = torch.load(OUTPUT_DIR+MODEL_ADD_PATH, map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final num of params = 5197870221\n"
     ]
    }
   ],
   "source": [
    "pruned_dict = torch.load(OUTPUT_DIR+MODEL_ADD_PATH, map_location=DEVICE)\n",
    "tokenizer, model = pruned_dict['tokenizer'], pruned_dict['model']\n",
    "\n",
    "print(f'Final num of params = {sum([np.prod(item.size())  for item in model.parameters()])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/trinity/home/team16/workspace/Compress-Code-LLMs-SMILES/human-eval/human_eval\n"
     ]
    }
   ],
   "source": [
    "%cd human-eval/human_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUMAN_EVAL =\"data/HumanEval.jsonl.gz\"\n",
    "\n",
    "def read_problems(evalset_file: str = HUMAN_EVAL) -> Dict[str, Dict]:\n",
    "    return {task[\"task_id\"]: task for task in stream_jsonl(evalset_file)}\n",
    "\n",
    "\n",
    "def stream_jsonl(filename: str) -> Iterable[Dict]:\n",
    "    \"\"\"\n",
    "    Parses each jsonl line and yields it as a dictionary\n",
    "    \"\"\"\n",
    "    if filename.endswith(\".gz\"):\n",
    "        with open(filename, \"rb\") as gzfp:\n",
    "            with gzip.open(gzfp, 'rt') as fp:\n",
    "                for line in fp:\n",
    "                    if any(not x.isspace() for x in line):\n",
    "                        yield json.loads(line)\n",
    "    else:\n",
    "        with open(filename, \"r\") as fp:\n",
    "            for line in fp:\n",
    "                if any(not x.isspace() for x in line):\n",
    "                    yield json.loads(line)\n",
    "\n",
    "\n",
    "def write_jsonl(filename: str, data: Iterable[Dict], append: bool = False):\n",
    "    \"\"\"\n",
    "    Writes an iterable of dictionaries to jsonl\n",
    "    \"\"\"\n",
    "    if append:\n",
    "        mode = 'ab'\n",
    "    else:\n",
    "        mode = 'wb'\n",
    "    filename = os.path.expanduser(filename)\n",
    "    if filename.endswith(\".gz\"):\n",
    "        with open(filename, mode) as fp:\n",
    "            with gzip.GzipFile(fileobj=fp, mode='wb') as gzfp:\n",
    "                for x in data:\n",
    "                    gzfp.write((json.dumps(x) + \"\\n\").encode('utf-8'))\n",
    "    else:\n",
    "        with open(filename, mode) as fp:\n",
    "            for x in data:\n",
    "                fp.write((json.dumps(x) + \"\\n\").encode('utf-8'))\n",
    "\n",
    "def generate_one_completion(prompt, max_new_tokens):\n",
    "    messages=[\n",
    "        { 'role': 'user', 'content': prompt}\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(inputs=inputs, max_new_tokens=max_new_tokens, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\n",
    "    out = (tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/trinity/home/team16/workspace/Compress-Code-LLMs-SMILES/human-eval\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/164 [00:00<?, ?it/s]/trinity/home/team16/workspace/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      "  1%|          | 1/164 [00:04<11:52,  4.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      "  1%|          | 2/164 [00:14<20:24,  7.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      "  2%|▏         | 3/164 [00:18<16:32,  6.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      "  2%|▏         | 4/164 [00:21<12:44,  4.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      "  3%|▎         | 5/164 [00:27<13:53,  5.24s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      "  4%|▎         | 6/164 [00:31<12:35,  4.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      "  4%|▍         | 7/164 [00:40<16:22,  6.26s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      "  5%|▍         | 8/164 [00:42<12:47,  4.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      "  5%|▌         | 9/164 [00:51<15:36,  6.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      "  6%|▌         | 10/164 [00:55<14:15,  5.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      "  7%|▋         | 11/164 [01:02<15:09,  5.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      "  7%|▋         | 12/164 [01:05<13:13,  5.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      "  8%|▊         | 13/164 [01:10<12:25,  4.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      "  9%|▊         | 14/164 [01:14<11:54,  4.76s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      "  9%|▉         | 15/164 [01:19<11:47,  4.75s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 10%|▉         | 16/164 [01:21<09:28,  3.84s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 10%|█         | 17/164 [01:23<08:30,  3.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 11%|█         | 18/164 [01:37<15:58,  6.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 13%|█▎        | 21/164 [02:14<28:42, 12.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 13%|█▎        | 22/164 [02:23<26:17, 11.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 14%|█▍        | 23/164 [02:25<19:36,  8.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 15%|█▍        | 24/164 [02:27<15:09,  6.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 15%|█▌        | 25/164 [02:31<13:27,  5.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 16%|█▌        | 26/164 [02:35<12:10,  5.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 16%|█▋        | 27/164 [02:39<11:27,  5.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 17%|█▋        | 28/164 [02:43<10:35,  4.67s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 18%|█▊        | 29/164 [02:45<08:15,  3.67s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 18%|█▊        | 30/164 [02:47<07:09,  3.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 19%|█▉        | 31/164 [02:48<05:52,  2.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 20%|█▉        | 32/164 [02:51<05:48,  2.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 20%|██        | 33/164 [03:01<11:00,  5.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 31%|███       | 51/164 [05:11<15:56,  8.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 32%|███▏      | 52/164 [05:15<13:13,  7.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 32%|███▏      | 53/164 [05:16<10:00,  5.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 33%|███▎      | 54/164 [05:17<07:31,  4.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 34%|███▎      | 55/164 [05:19<06:27,  3.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 34%|███▍      | 56/164 [05:22<05:35,  3.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 35%|███▍      | 57/164 [05:27<06:38,  3.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 35%|███▌      | 58/164 [05:29<05:50,  3.30s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 36%|███▌      | 59/164 [05:34<06:55,  3.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 37%|███▋      | 60/164 [05:38<06:52,  3.96s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 37%|███▋      | 61/164 [05:44<07:49,  4.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 38%|███▊      | 62/164 [05:50<08:09,  4.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 38%|███▊      | 63/164 [05:57<09:24,  5.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 39%|███▉      | 64/164 [06:01<08:14,  4.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 40%|███▉      | 65/164 [06:04<07:13,  4.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 40%|████      | 66/164 [06:12<09:12,  5.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 41%|████      | 67/164 [06:14<07:14,  4.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 41%|████▏     | 68/164 [06:17<06:25,  4.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 42%|████▏     | 69/164 [06:39<15:07,  9.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 43%|████▎     | 70/164 [06:45<13:03,  8.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 43%|████▎     | 71/164 [06:48<10:40,  6.88s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 44%|████▍     | 72/164 [06:52<09:13,  6.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 45%|████▍     | 73/164 [07:18<18:06, 11.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 45%|████▌     | 74/164 [07:27<16:34, 11.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 46%|████▌     | 75/164 [07:33<14:12,  9.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 46%|████▋     | 76/164 [07:36<11:04,  7.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 47%|████▋     | 77/164 [07:39<08:57,  6.18s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 48%|████▊     | 78/164 [07:41<06:52,  4.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 48%|████▊     | 79/164 [08:07<15:45, 11.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 49%|████▉     | 80/164 [08:10<12:11,  8.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 49%|████▉     | 81/164 [08:13<10:00,  7.24s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 50%|█████     | 82/164 [08:38<17:02, 12.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 51%|█████     | 83/164 [08:41<13:05,  9.70s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 51%|█████     | 84/164 [08:44<10:14,  7.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 52%|█████▏    | 85/164 [08:55<11:13,  8.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 52%|█████▏    | 86/164 [09:00<09:42,  7.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 53%|█████▎    | 87/164 [09:09<10:18,  8.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 54%|█████▎    | 88/164 [09:29<14:38, 11.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 54%|█████▍    | 89/164 [09:55<19:47, 15.84s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 55%|█████▍    | 90/164 [10:00<15:24, 12.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 55%|█████▌    | 91/164 [10:03<12:01,  9.88s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 56%|█████▌    | 92/164 [10:07<09:29,  7.91s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 57%|█████▋    | 93/164 [10:32<15:41, 13.26s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 57%|█████▋    | 94/164 [10:36<12:10, 10.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 58%|█████▊    | 95/164 [11:02<17:20, 15.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 59%|█████▊    | 96/164 [11:06<13:26, 11.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 59%|█████▉    | 97/164 [11:10<10:31,  9.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 60%|█████▉    | 98/164 [11:13<08:11,  7.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 60%|██████    | 99/164 [11:16<06:34,  6.07s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 61%|██████    | 100/164 [11:18<05:19,  5.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 62%|██████▏   | 101/164 [11:31<07:31,  7.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 62%|██████▏   | 102/164 [11:32<05:29,  5.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 63%|██████▎   | 103/164 [11:38<05:44,  5.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 63%|██████▎   | 104/164 [11:41<04:44,  4.74s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 64%|██████▍   | 105/164 [11:50<05:57,  6.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 65%|██████▍   | 106/164 [12:04<08:11,  8.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 65%|██████▌   | 107/164 [12:07<06:39,  7.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 66%|██████▌   | 108/164 [12:24<09:04,  9.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 66%|██████▋   | 109/164 [12:26<06:51,  7.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 67%|██████▋   | 110/164 [12:30<05:50,  6.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 68%|██████▊   | 111/164 [12:38<06:09,  6.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 68%|██████▊   | 112/164 [12:49<06:58,  8.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 69%|██████▉   | 113/164 [12:52<05:33,  6.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 70%|██████▉   | 114/164 [13:01<06:07,  7.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 70%|███████   | 115/164 [13:09<06:16,  7.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 71%|███████   | 116/164 [13:28<08:49, 11.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 71%|███████▏  | 117/164 [13:31<06:42,  8.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 72%|███████▏  | 118/164 [13:34<05:15,  6.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 73%|███████▎  | 119/164 [13:59<09:19, 12.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 73%|███████▎  | 120/164 [14:25<11:57, 16.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 74%|███████▍  | 121/164 [14:38<11:03, 15.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 74%|███████▍  | 122/164 [14:40<08:04, 11.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 75%|███████▌  | 123/164 [14:52<07:49, 11.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 76%|███████▌  | 124/164 [14:57<06:18,  9.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 76%|███████▌  | 125/164 [15:22<09:15, 14.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 77%|███████▋  | 126/164 [15:23<06:33, 10.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 77%|███████▋  | 127/164 [15:26<04:56,  8.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 78%|███████▊  | 128/164 [15:51<07:56, 13.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 79%|███████▊  | 129/164 [15:53<05:44,  9.84s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 79%|███████▉  | 130/164 [16:15<07:40, 13.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 80%|███████▉  | 131/164 [16:19<05:52, 10.67s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 80%|████████  | 132/164 [16:22<04:24,  8.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 81%|████████  | 133/164 [16:25<03:31,  6.83s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 82%|████████▏ | 134/164 [16:34<03:36,  7.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 82%|████████▏ | 135/164 [16:37<02:53,  5.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 83%|████████▎ | 136/164 [16:40<02:21,  5.07s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 84%|████████▎ | 137/164 [16:45<02:21,  5.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 84%|████████▍ | 138/164 [17:11<04:53, 11.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 85%|████████▍ | 139/164 [17:19<04:20, 10.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 85%|████████▌ | 140/164 [17:26<03:45,  9.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 86%|████████▌ | 141/164 [17:30<03:01,  7.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 87%|████████▋ | 142/164 [17:35<02:29,  6.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 87%|████████▋ | 143/164 [17:46<02:53,  8.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 88%|████████▊ | 144/164 [17:50<02:15,  6.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 88%|████████▊ | 145/164 [17:58<02:15,  7.15s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 89%|████████▉ | 146/164 [18:03<01:56,  6.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 90%|████████▉ | 147/164 [18:05<01:30,  5.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 90%|█████████ | 148/164 [18:08<01:13,  4.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 91%|█████████ | 149/164 [18:33<02:42, 10.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 91%|█████████▏| 150/164 [18:37<01:59,  8.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 92%|█████████▏| 151/164 [19:02<02:56, 13.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 93%|█████████▎| 152/164 [19:09<02:20, 11.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 93%|█████████▎| 153/164 [19:23<02:15, 12.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 94%|█████████▍| 154/164 [19:38<02:09, 12.96s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 95%|█████████▍| 155/164 [19:42<01:34, 10.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 95%|█████████▌| 156/164 [19:47<01:09,  8.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 96%|█████████▌| 157/164 [20:12<01:35, 13.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 96%|█████████▋| 158/164 [20:16<01:04, 10.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 97%|█████████▋| 159/164 [20:21<00:44,  8.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 98%|█████████▊| 160/164 [20:25<00:30,  7.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 98%|█████████▊| 161/164 [20:51<00:38, 12.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 99%|█████████▉| 162/164 [20:54<00:20, 10.13s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      " 99%|█████████▉| 163/164 [20:56<00:07,  7.59s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n",
      "100%|██████████| 164/164 [20:59<00:00,  7.68s/it]\n"
     ]
    }
   ],
   "source": [
    "problems = read_problems()\n",
    "\n",
    "samples = [\n",
    "    dict(task_id=task_id, completion=generate_one_completion(problems[task_id][\"prompt\"], max_new_tokens=MAX_NEW_TOKENS))\n",
    "    for task_id in tqdm(problems)\n",
    "    for _ in range(NUM_SAMPLES_PER_TASK)\n",
    "]\n",
    "write_jsonl(SAVE_METRIC_PATH, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/trinity/home/team16/workspace/Compress-Code-LLMs-SMILES/human-eval/human_eval\n"
     ]
    }
   ],
   "source": [
    "%cd human_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "164it [00:00, 35114.90it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 266.72it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 53580.45it/s]\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import json\n",
    "\n",
    "out = subprocess.run(['python', 'evaluate_functional_correctness.py','../'+SAVE_METRIC_PATH],stdout=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = str(out.stdout).split('{')[-1].split('}')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = {}\n",
    "metrics = metrics.split(',')\n",
    "for metric in metrics:\n",
    "    k, val = metric.split(':')\n",
    "    metrics_dict[k[1:][:-1]] = float(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pass@1': 0.15853658536585366}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
